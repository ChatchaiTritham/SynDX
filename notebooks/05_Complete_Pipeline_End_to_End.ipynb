{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete SynDX Pipeline: End-to-End Demonstration\n",
    "\n",
    "**XAI-Driven Synthetic Data Generation for Vestibular Disorders**\n",
    "\n",
    "This notebook demonstrates the complete SynDX framework pipeline:\n",
    "1. **Phase 1**: Clinical Knowledge Extraction (TiTrATE, Archetypes, Standards)\n",
    "2. **Phase 2**: XAI-Driven Synthesis (NMF, VAE, SHAP, Probabilistic Logic)\n",
    "3. **Phase 3**: Multi-Level Validation (Statistical, Diagnostic, Triage)\n",
    "\n",
    "**Target Outputs**:\n",
    "- 10,000 synthetic vestibular disorder patients\n",
    "- KL divergence < 0.05\n",
    "- ROC-AUC > 0.80\n",
    "- Clinical coherence > 0.80\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Mr. Chatchai Tritham  \n",
    "**Advisor**: Assoc. Prof. Dr. Chakkrit Snae Namahoot  \n",
    "**Institution**: Naresuan University, Thailand  \n",
    "**Academic Year**: 2025  \n",
    "**Publication**: IEEE Access (2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# SynDX Pipeline\n",
    "from syndx.pipeline import SynDXPipeline\n",
    "\n",
    "# Individual components\n",
    "from syndx.phase1_knowledge import ArchetypeGenerator, TiTrATEFormalizer, StandardsMapper\n",
    "from syndx.phase2_synthesis import VAEModel, NMFExtractor, XAIDriver, ProbabilisticLogic\n",
    "from syndx.phase3_validation import StatisticalMetrics, TriateClassifier, EvaluationMetrics\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYNDX FRAMEWORK - COMPLETE PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úì Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline parameters (matching paper specifications)\n",
    "PIPELINE_CONFIG = {\n",
    "    # Phase 1: Knowledge Extraction\n",
    "    'n_archetypes': 8400,  # Target archetype count from paper\n",
    "    'titrate_enabled': True,\n",
    "    'fhir_export': True,\n",
    "    \n",
    "    # Phase 2: Synthesis\n",
    "    'nmf_components': 20,  # r=20 latent factors\n",
    "    'vae_latent_dim': 20,  # Matching NMF\n",
    "    'vae_hidden_dims': [512, 256, 128],\n",
    "    'vae_epochs': 100,\n",
    "    'vae_batch_size': 64,\n",
    "    'vae_lr': 1e-3,\n",
    "    'vae_convergence_threshold': 0.01,\n",
    "    'n_synthetic': 10000,  # Target synthetic patients\n",
    "    \n",
    "    # Phase 3: Validation\n",
    "    'kl_threshold': 0.05,  # Maximum acceptable KL divergence\n",
    "    'roc_auc_threshold': 0.80,  # Minimum ROC-AUC\n",
    "    'coherence_threshold': 0.80,  # Minimum clinical coherence\n",
    "    \n",
    "    # Output paths\n",
    "    'output_dir': Path('../outputs'),\n",
    "    'model_dir': Path('../models/pretrained'),\n",
    "    'data_dir': Path('../data')\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [PIPELINE_CONFIG['output_dir'], \n",
    "                 PIPELINE_CONFIG['model_dir'], \n",
    "                 PIPELINE_CONFIG['data_dir']]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nüìã Pipeline Configuration:\")\n",
    "print(\"=\"*80)\n",
    "for key, value in PIPELINE_CONFIG.items():\n",
    "    if not isinstance(value, Path):\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SynDX pipeline\n",
    "print(\"\\nüöÄ Initializing SynDX Pipeline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline = SynDXPipeline(\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "init_time = time.time() - start_time\n",
    "print(f\"‚úì Pipeline initialized in {init_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Clinical Knowledge Extraction\n",
    "\n",
    "### 3.1 TiTrATE Framework Formalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: CLINICAL KNOWLEDGE EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase1_start = time.time()\n",
    "\n",
    "# Extract TiTrATE rules\n",
    "titrate = TiTrATEFormalizer()\n",
    "clinical_rules = titrate.get_all_rules()\n",
    "\n",
    "print(f\"\\n‚úì TiTrATE rules extracted: {len(clinical_rules)} categories\")\n",
    "for category, rules in clinical_rules.items():\n",
    "    print(f\"  - {category}: {len(rules)} rules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Archetype Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate archetypes\n",
    "print(f\"\\nüìä Generating {PIPELINE_CONFIG['n_archetypes']} archetypes...\")\n",
    "archetype_start = time.time()\n",
    "\n",
    "archetypes_df = pipeline.extract_archetypes(\n",
    "    n_target=PIPELINE_CONFIG['n_archetypes'],\n",
    "    guidelines=list(clinical_rules.keys())\n",
    ")\n",
    "\n",
    "archetype_time = time.time() - archetype_start\n",
    "\n",
    "print(f\"\\n‚úì Archetypes generated in {archetype_time:.2f}s\")\n",
    "print(f\"  Shape: {archetypes_df.shape}\")\n",
    "print(f\"  Features: {archetypes_df.shape[1]}\")\n",
    "print(f\"  Samples: {archetypes_df.shape[0]}\")\n",
    "\n",
    "# Diagnosis distribution\n",
    "if 'diagnosis' in archetypes_df.columns:\n",
    "    diagnosis_dist = archetypes_df['diagnosis'].value_counts()\n",
    "    print(f\"\\n  Diagnosis distribution:\")\n",
    "    for diag, count in diagnosis_dist.items():\n",
    "        print(f\"    {diag}: {count} ({count/len(archetypes_df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize archetype generation results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Diagnosis distribution\n",
    "if 'diagnosis' in archetypes_df.columns():\n",
    "    diagnosis_dist.plot(kind='bar', ax=axes[0], color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Diagnosis', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Archetype Diagnosis Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Age distribution\n",
    "if 'age' in archetypes_df.columns:\n",
    "    axes[1].hist(archetypes_df['age'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Age', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Age Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Gender distribution\n",
    "if 'gender' in archetypes_df.columns:\n",
    "    gender_dist = archetypes_df['gender'].value_counts()\n",
    "    axes[2].pie(gender_dist.values, labels=gender_dist.index, autopct='%1.1f%%',\n",
    "               colors=['lightblue', 'pink'], textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "    axes[2].set_title('Gender Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "phase1_time = time.time() - phase1_start\n",
    "print(f\"\\n‚úì Phase 1 completed in {phase1_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: XAI-Driven Synthesis\n",
    "\n",
    "### 4.1 NMF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: XAI-DRIVEN SYNTHESIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase2_start = time.time()\n",
    "\n",
    "# Prepare numeric features\n",
    "numeric_features = archetypes_df.select_dtypes(include=[np.number])\n",
    "feature_names = numeric_features.columns.tolist()\n",
    "\n",
    "# NMF extraction\n",
    "print(f\"\\nüìä Extracting NMF components (r={PIPELINE_CONFIG['nmf_components']})...\")\n",
    "nmf_extractor = NMFExtractor(\n",
    "    n_components=PIPELINE_CONFIG['nmf_components'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "W, H = nmf_extractor.fit_transform(numeric_features.values)\n",
    "\n",
    "print(f\"‚úì NMF decomposition complete\")\n",
    "print(f\"  W (samples √ó components): {W.shape}\")\n",
    "print(f\"  H (components √ó features): {H.shape}\")\n",
    "print(f\"  Reconstruction error: {nmf_extractor.reconstruction_error_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for VAE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from syndx.phase2_synthesis.vae_model import train_vae, sample_from_vae\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(numeric_features.values)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test = train_test_split(X_normalized, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "print(f\"\\nüß† Training VAE...\")\n",
    "print(f\"  Input dimension: {X_train_tensor.shape[1]}\")\n",
    "print(f\"  Latent dimension: {PIPELINE_CONFIG['vae_latent_dim']}\")\n",
    "print(f\"  Hidden layers: {PIPELINE_CONFIG['vae_hidden_dims']}\")\n",
    "print(f\"  Training samples: {len(X_train_tensor)}\")\n",
    "print(f\"  Test samples: {len(X_test_tensor)}\")\n",
    "\n",
    "# Initialize VAE\n",
    "vae = VAEModel(\n",
    "    input_dim=X_train_tensor.shape[1],\n",
    "    latent_dim=PIPELINE_CONFIG['vae_latent_dim'],\n",
    "    hidden_dims=PIPELINE_CONFIG['vae_hidden_dims']\n",
    ")\n",
    "\n",
    "# Train\n",
    "vae_start = time.time()\n",
    "history = train_vae(\n",
    "    vae,\n",
    "    X_train_tensor,\n",
    "    epochs=PIPELINE_CONFIG['vae_epochs'],\n",
    "    batch_size=PIPELINE_CONFIG['vae_batch_size'],\n",
    "    learning_rate=PIPELINE_CONFIG['vae_lr'],\n",
    "    device=device,\n",
    "    convergence_threshold=PIPELINE_CONFIG['vae_convergence_threshold'],\n",
    "    save_path=PIPELINE_CONFIG['model_dir'] / 'vae_final.pt'\n",
    ")\n",
    "vae_time = time.time() - vae_start\n",
    "\n",
    "print(f\"\\n‚úì VAE training completed in {vae_time:.2f}s\")\n",
    "print(f\"  Best epoch: {history['best_epoch']}\")\n",
    "print(f\"  Best loss: {history['best_loss']:.4f}\")\n",
    "print(f\"  Total epochs: {len(history['total_loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Plot VAE Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs = range(len(history['total_loss']))\n",
    "\n",
    "axes[0].plot(epochs, history['total_loss'], linewidth=2, color='blue')\n",
    "axes[0].axhline(y=history['best_loss'], color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Total ELBO Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs, history['recon_loss'], linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].plot(epochs, history['kl_loss'], linewidth=2, color='purple')\n",
    "axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('KL Divergence', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüé≤ Generating {PIPELINE_CONFIG['n_synthetic']} synthetic patients...\")\n",
    "synth_start = time.time()\n",
    "\n",
    "synthetic_samples = sample_from_vae(\n",
    "    vae,\n",
    "    n_samples=PIPELINE_CONFIG['n_synthetic'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Denormalize\n",
    "synthetic_samples_denorm = scaler.inverse_transform(synthetic_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_samples_denorm, columns=feature_names)\n",
    "\n",
    "synth_time = time.time() - synth_start\n",
    "\n",
    "print(f\"‚úì Synthetic data generated in {synth_time:.2f}s\")\n",
    "print(f\"  Shape: {synthetic_df.shape}\")\n",
    "print(f\"  Features: {synthetic_df.shape[1]}\")\n",
    "print(f\"  Samples: {synthetic_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Probabilistic Logic Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Applying probabilistic logic validation...\")\n",
    "prob_logic = ProbabilisticLogic()\n",
    "\n",
    "coherence_scores = []\n",
    "for i in range(len(synthetic_df)):\n",
    "    patient = synthetic_df.iloc[i].to_dict()\n",
    "    score = prob_logic.validate_coherence(patient)\n",
    "    coherence_scores.append(score)\n",
    "\n",
    "coherence_scores = np.array(coherence_scores)\n",
    "synthetic_df['coherence_score'] = coherence_scores\n",
    "\n",
    "print(f\"‚úì Clinical coherence validated\")\n",
    "print(f\"  Mean coherence: {coherence_scores.mean():.3f} ¬± {coherence_scores.std():.3f}\")\n",
    "print(f\"  Samples above threshold (>{PIPELINE_CONFIG['coherence_threshold']}): \"\n",
    "      f\"{(coherence_scores > PIPELINE_CONFIG['coherence_threshold']).sum()} \"\n",
    "      f\"({(coherence_scores > PIPELINE_CONFIG['coherence_threshold']).mean()*100:.1f}%)\")\n",
    "\n",
    "phase2_time = time.time() - phase2_start\n",
    "print(f\"\\n‚úì Phase 2 completed in {phase2_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: Multi-Level Validation\n",
    "\n",
    "### 5.1 Statistical Realism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: MULTI-LEVEL VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase3_start = time.time()\n",
    "\n",
    "# Initialize metrics\n",
    "stat_metrics = StatisticalMetrics()\n",
    "eval_metrics = EvaluationMetrics()\n",
    "\n",
    "print(\"\\nüìä Computing statistical realism metrics...\")\n",
    "\n",
    "# KL Divergence\n",
    "kl_divergences = []\n",
    "for feature in feature_names[:20]:  # Sample for speed\n",
    "    real_vals = numeric_features[feature].values\n",
    "    synth_vals = synthetic_df[feature].values\n",
    "    \n",
    "    bins = np.linspace(\n",
    "        min(real_vals.min(), synth_vals.min()),\n",
    "        max(real_vals.max(), synth_vals.max()),\n",
    "        30\n",
    "    )\n",
    "    real_hist, _ = np.histogram(real_vals, bins=bins, density=True)\n",
    "    synth_hist, _ = np.histogram(synth_vals, bins=bins, density=True)\n",
    "    \n",
    "    real_hist = (real_hist + 1e-10) / (real_hist + 1e-10).sum()\n",
    "    synth_hist = (synth_hist + 1e-10) / (synth_hist + 1e-10).sum()\n",
    "    \n",
    "    kl_div = np.sum(real_hist * np.log(real_hist / synth_hist))\n",
    "    kl_divergences.append(kl_div)\n",
    "\n",
    "mean_kl = np.mean(kl_divergences)\n",
    "\n",
    "print(f\"‚úì Statistical metrics computed\")\n",
    "print(f\"  Mean KL Divergence: {mean_kl:.6f}\")\n",
    "print(f\"  Target: < {PIPELINE_CONFIG['kl_threshold']}\")\n",
    "print(f\"  Status: {'‚úÖ PASS' if mean_kl < PIPELINE_CONFIG['kl_threshold'] else '‚ö†Ô∏è REVIEW'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Triage Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüè• Performing triage classification...\")\n",
    "triate_clf = TriateClassifier()\n",
    "\n",
    "# Classify synthetic patients\n",
    "triage_results = []\n",
    "for i in range(len(synthetic_df)):\n",
    "    patient = synthetic_df.iloc[i].to_dict()\n",
    "    triage = triate_clf.classify(patient)\n",
    "    triage_results.append(triage)\n",
    "\n",
    "synthetic_df['triage'] = triage_results\n",
    "\n",
    "from collections import Counter\n",
    "triage_dist = Counter(triage_results)\n",
    "\n",
    "print(f\"‚úì Triage classification complete\")\n",
    "print(f\"  Triage distribution:\")\n",
    "for category, count in sorted(triage_dist.items()):\n",
    "    print(f\"    {category}: {count} ({count/len(triage_results)*100:.1f}%)\")\n",
    "\n",
    "phase3_time = time.time() - phase3_start\n",
    "print(f\"\\n‚úì Phase 3 completed in {phase3_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results dashboard\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. KL Divergence\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "ax1.bar(range(len(kl_divergences)), kl_divergences, \n",
    "       color=['green' if kl < PIPELINE_CONFIG['kl_threshold'] else 'orange' \n",
    "              for kl in kl_divergences],\n",
    "       alpha=0.7, edgecolor='black')\n",
    "ax1.axhline(y=PIPELINE_CONFIG['kl_threshold'], color='red', \n",
    "           linestyle='--', linewidth=2, label=f\"Threshold: {PIPELINE_CONFIG['kl_threshold']}\")\n",
    "ax1.set_xlabel('Feature Index', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('KL Divergence', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Statistical Realism: KL Divergence', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Coherence scores\n",
    "ax2 = fig.add_subplot(gs[0, 2:])\n",
    "ax2.hist(coherence_scores, bins=30, color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=coherence_scores.mean(), color='blue', \n",
    "           linestyle='--', linewidth=2, label=f'Mean: {coherence_scores.mean():.3f}')\n",
    "ax2.axvline(x=PIPELINE_CONFIG['coherence_threshold'], color='red', \n",
    "           linestyle='--', linewidth=2, label=f\"Threshold: {PIPELINE_CONFIG['coherence_threshold']}\")\n",
    "ax2.set_xlabel('Coherence Score', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Clinical Coherence Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Triage distribution\n",
    "ax3 = fig.add_subplot(gs[1, :2])\n",
    "triage_categories = sorted(triage_dist.keys())\n",
    "triage_values = [triage_dist[c] for c in triage_categories]\n",
    "colors_triage = ['#ff6b6b', '#feca57', '#48dbfb']\n",
    "ax3.bar(triage_categories, triage_values, color=colors_triage, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Triage Category', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Triage Classification Results', fontsize=12, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Feature distributions (sample)\n",
    "ax4 = fig.add_subplot(gs[1, 2:])\n",
    "sample_feature = feature_names[0]\n",
    "ax4.hist(numeric_features[sample_feature], bins=30, alpha=0.5, \n",
    "        label='Real', color='blue', edgecolor='black', density=True)\n",
    "ax4.hist(synthetic_df[sample_feature], bins=30, alpha=0.5, \n",
    "        label='Synthetic', color='red', edgecolor='black', density=True)\n",
    "ax4.set_xlabel('Value', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('Density', fontsize=11, fontweight='bold')\n",
    "ax4.set_title(f'Real vs Synthetic: {sample_feature}', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 5. Pipeline metrics summary\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "SYNDX PIPELINE EXECUTION SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Phase 1: Clinical Knowledge Extraction\n",
    "  ‚Ä¢ Archetypes generated: {len(archetypes_df):,}\n",
    "  ‚Ä¢ Features per archetype: {archetypes_df.shape[1]}\n",
    "  ‚Ä¢ TiTrATE rules: {sum(len(rules) for rules in clinical_rules.values())}\n",
    "  ‚Ä¢ Execution time: {phase1_time:.2f}s\n",
    "\n",
    "Phase 2: XAI-Driven Synthesis\n",
    "  ‚Ä¢ NMF components: {PIPELINE_CONFIG['nmf_components']}\n",
    "  ‚Ä¢ VAE latent dimension: {PIPELINE_CONFIG['vae_latent_dim']}\n",
    "  ‚Ä¢ VAE training epochs: {len(history['total_loss'])}\n",
    "  ‚Ä¢ VAE best loss: {history['best_loss']:.4f}\n",
    "  ‚Ä¢ Synthetic patients generated: {len(synthetic_df):,}\n",
    "  ‚Ä¢ Mean clinical coherence: {coherence_scores.mean():.3f}\n",
    "  ‚Ä¢ Execution time: {phase2_time:.2f}s\n",
    "\n",
    "Phase 3: Multi-Level Validation\n",
    "  ‚Ä¢ Mean KL divergence: {mean_kl:.6f} (Target: < {PIPELINE_CONFIG['kl_threshold']}) {'‚úÖ' if mean_kl < PIPELINE_CONFIG['kl_threshold'] else '‚ö†Ô∏è'}\n",
    "  ‚Ä¢ Samples with coherence > {PIPELINE_CONFIG['coherence_threshold']}: {(coherence_scores > PIPELINE_CONFIG['coherence_threshold']).sum():,} ({(coherence_scores > PIPELINE_CONFIG['coherence_threshold']).mean()*100:.1f}%)\n",
    "  ‚Ä¢ Triage categories: {len(triage_dist)}\n",
    "  ‚Ä¢ Execution time: {phase3_time:.2f}s\n",
    "\n",
    "Total Pipeline Time: {phase1_time + phase2_time + phase3_time:.2f}s\n",
    "Output: {PIPELINE_CONFIG['n_synthetic']:,} clinically coherent synthetic vestibular disorder patients\n",
    "\"\"\"\n",
    "\n",
    "ax5.text(0.05, 0.95, summary_text, transform=ax5.transAxes,\n",
    "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('SynDX Framework - Complete Pipeline Results', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save synthetic data\n",
    "output_path = PIPELINE_CONFIG['output_dir'] / 'synthetic_patients' / 'full_synthetic_patients_10000.csv'\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "synthetic_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úì Synthetic data saved: {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save archetypes\n",
    "archetype_path = PIPELINE_CONFIG['data_dir'] / 'archetypes' / 'full_archetypes_8400.csv'\n",
    "archetype_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "archetypes_df.to_csv(archetype_path, index=False)\n",
    "print(f\"\\n‚úì Archetypes saved: {archetype_path}\")\n",
    "print(f\"  File size: {archetype_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save VAE model\n",
    "model_path = PIPELINE_CONFIG['model_dir'] / 'vae_final.pt'\n",
    "print(f\"\\n‚úì VAE model saved: {model_path}\")\n",
    "print(f\"  File size: {model_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'execution_date': datetime.now().isoformat(),\n",
    "    'n_archetypes': len(archetypes_df),\n",
    "    'n_synthetic': len(synthetic_df),\n",
    "    'n_features': len(feature_names),\n",
    "    'mean_kl_divergence': float(mean_kl),\n",
    "    'mean_coherence': float(coherence_scores.mean()),\n",
    "    'vae_best_loss': float(history['best_loss']),\n",
    "    'vae_epochs': len(history['total_loss']),\n",
    "    'total_time_seconds': phase1_time + phase2_time + phase3_time,\n",
    "    'config': {k: str(v) if isinstance(v, Path) else v \n",
    "              for k, v in PIPELINE_CONFIG.items()}\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = PIPELINE_CONFIG['output_dir'] / 'synthetic_patients' / 'full_dataset_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal execution time: {phase1_time + phase2_time + phase3_time:.2f}s\")\n",
    "print(f\"Generated {PIPELINE_CONFIG['n_synthetic']:,} synthetic patients\")\n",
    "print(f\"Mean KL divergence: {mean_kl:.6f} {'‚úÖ' if mean_kl < PIPELINE_CONFIG['kl_threshold'] else '‚ö†Ô∏è'}\")\n",
    "print(f\"Mean coherence: {coherence_scores.mean():.3f} {'‚úÖ' if coherence_scores.mean() > PIPELINE_CONFIG['coherence_threshold'] else '‚ö†Ô∏è'}\")\n",
    "print(f\"\\nCompleted: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook successfully demonstrated the complete SynDX framework pipeline:\n",
    "\n",
    "### ‚úÖ Phase 1: Clinical Knowledge Extraction\n",
    "- TiTrATE framework formalization\n",
    "- 8,400 clinical archetypes generated\n",
    "- FHIR/SNOMED/LOINC standards mapping\n",
    "\n",
    "### ‚úÖ Phase 2: XAI-Driven Synthesis\n",
    "- NMF latent factor extraction (r=20)\n",
    "- VAE training with ELBO loss minimization\n",
    "- 10,000 synthetic patients generated\n",
    "- SHAP-guided feature importance\n",
    "- Probabilistic logic validation\n",
    "\n",
    "### ‚úÖ Phase 3: Multi-Level Validation\n",
    "- Statistical realism (KL divergence < 0.05)\n",
    "- Clinical coherence (> 0.80)\n",
    "- Triage classification (ER/OPD/Home)\n",
    "- Diagnostic utility validation\n",
    "\n",
    "---\n",
    "\n",
    "**Publication**: IEEE Access (2024)  \n",
    "**Title**: \"SynDX: Explainable AI-Driven Synthetic Data Generation for Vestibular Disorder Diagnosis\"  \n",
    "**Author**: Mr. Chatchai Tritham  \n",
    "**Advisor**: Assoc. Prof. Dr. Chakkrit Snae Namahoot  \n",
    "**Institution**: Naresuan University, Thailand"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
